{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Camera Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from ..\\CarND-Project4\\CarND-Advanced-Lane-Lines-master\\camera_cal  read in chessboard images and find corners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#code segment adapted from code presented in OpenCV-python-tutorials\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# path name for folder storing camera calibration images\n",
    "pathname = 'C:\\\\Users\\\\meile\\\\Projects\\\\CarND-Project4\\\\CarND-Advanced-Lane-Lines-master\\\\camera_cal\\\\'\n",
    "\n",
    "# termination criteria\n",
    "criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)\n",
    "\n",
    "# prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)\n",
    "nx = 9\n",
    "ny = 6\n",
    "\n",
    "objp = np.zeros((ny*nx,3), np.float32)\n",
    "objp[:,:2] = np.mgrid[0:nx,0:ny].T.reshape(-1,2)\n",
    "\n",
    "# Arrays to store object points and image points from all the images.\n",
    "objpoints = [] # 3d point in real world space\n",
    "imgpoints = [] # 2d points in image plane.\n",
    "\n",
    "images = glob.glob(pathname+'*.jpg')\n",
    "\n",
    "def corners_unwarp(images, nx, ny):\n",
    "\n",
    "    for fname in images:\n",
    "        img = cv2.imread(fname)\n",
    "        gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        img_size = gray.shape[::-1]\n",
    "    \n",
    "        # step 1: capture the possible chess board corners using cv2.findChessboardCorners function\n",
    "        ret, corners = cv2.findChessboardCorners(gray, (nx,ny),None)\n",
    "\n",
    "        # step 2: check to see if any corners are found, add object points, image points (after refining them)\n",
    "        if ret == True:\n",
    "            objpoints.append(objp)\n",
    "\n",
    "            corners2 = cv2.cornerSubPix(gray,corners,(11,11),(-1,-1),criteria)\n",
    "            imgpoints.append(corners2)\n",
    "\n",
    "            # Draw and display the corners on the image used to search for the presence of corners\n",
    "            img = cv2.drawChessboardCorners(img, (nx,ny), corners2,ret)\n",
    "            save_name = fname + \"copy.jpg\"  #<-- this is a dirty workaround for image renaming.\n",
    "            \n",
    "            cv2.imwrite(save_name,img)\n",
    "\n",
    "            #also taken from Open-CV-python tutorials, same as what is used in the Udacity class materials\n",
    "            #Processing of step 2.  \n",
    "            \n",
    "            #So now we have our object points and image points we are ready to go for calibration. \n",
    "            #For that we use the function, cv2.calibrateCamera(). It returns the camera matrix, distortion coefficients, \n",
    "            #rotation and translation vectors etc.\n",
    "            ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, gray.shape[::-1],None,None)\n",
    "\n",
    "            h,  w = img.shape[:2]\n",
    "            newcameramtx, roi=cv2.getOptimalNewCameraMatrix(mtx,dist,(w,h),1,(w,h))\n",
    "        \n",
    "            undist = cv2.undistort(img, newcameramtx, dist, None, mtx)\n",
    "        \n",
    "            src = np.float32([corners[0],corners[nx-1],corners[-1],corners[-nx]])\n",
    "            offset = 100\n",
    "            # define 4 destination points dst = np.float32([[,],[,],[,],[,]])\n",
    "            dst = np.float32([[offset, offset],[img_size[0]-offset,offset],[img_size[0]-offset,img_size[1]-offset],[offset, img_size[1] - offset]])\n",
    "            # use cv2.getPerspectiveTransform() to get M, the transform matrix\n",
    "            M = cv2.getPerspectiveTransform(src, dst)\n",
    "            Minv = cv2.getPerspectiveTransform(dst, src)\n",
    "            # use cv2.warpPerspective() to warp your image to a top-down view\n",
    "            warped_input = cv2.warpPerspective(undist, M, img_size, flags=cv2.INTER_LINEAR)\n",
    "\n",
    "    return warped_input, M, newcameramtx, dist, mtx\n",
    "\n",
    "#unwraping the image, into a warped image.  All variables are renamed to identify the camera related images, not the road images\n",
    "warped_cam, M_cam, newcameramtx_cam, dist_cam, mtx_cam= corners_unwarp(images, nx, ny)\n",
    "\n",
    "save_name_warped = pathname+'warped_cam_img.jpg'\n",
    "\n",
    "cv2.imwrite(save_name_warped, warped_cam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline (test images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # undistorting test images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # # Provide an example of a distortion-corrected image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Undistorting Test images ####\n",
    "\n",
    "def correct_dist(fname, newcameramtx, dist, mtx):\n",
    "    img2 = cv2.imread(fname)\n",
    "    gray2 = cv2.cvtColor(img2,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    undist2 = cv2.undistort(img2, newcameramtx, dist, None, mtx)\n",
    "\n",
    "    return img2, undist2\n",
    "\n",
    "def correct_dist_video(img2, newcameramtx, dist, mtx):\n",
    "    \n",
    "    gray2 = cv2.cvtColor(img2,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    undist2 = cv2.undistort(img2, newcameramtx, dist, None, mtx)\n",
    "\n",
    "    return img2, undist2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # # Describe how (and identify where in your code) you used color transforms, gradients or other methods to create a thresholded binary image. Provide an example of a binary image result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "plt.close('all') # Close a figure window\n",
    "\n",
    "sobel_kernel=3\n",
    "#mag_thresh=(30, 255)\n",
    "s_thresh=(220, 255) #(170, 255) \n",
    "sx_thresh=(30,100)#(20, 100)\n",
    "\n",
    "def pipeline(img2, s_thresh, sx_thresh):\n",
    "\n",
    "    # from course material.  Apply the following steps to img2\n",
    "    # 1) Convert to HSV color space and separate the V channel\n",
    "    hsv = cv2.cvtColor(img2, cv2.COLOR_RGB2HSV).astype(np.float)\n",
    "    v_channel = hsv[:,:,1]\n",
    "    s_channel = hsv[:,:,2]\n",
    "    \n",
    "    # 2) Take the gradient in x + y\n",
    "    sobelx = cv2.Sobel(s_channel, cv2.CV_64F, 1, 0) # Take the derivative in x\n",
    "    sobely = cv2.Sobel(s_channel, cv2.CV_64F, 0, 1, ksize=sobel_kernel)\n",
    "    sobelx_glare = cv2.Sobel(v_channel, cv2.CV_64F, 1, 0) # Take the derivative in x\n",
    "    sobely_glare = cv2.Sobel(v_channel, cv2.CV_64F, 0, 1, ksize=sobel_kernel)\n",
    "    \n",
    "    # 3) Calculate the magnitude \n",
    "    sqrt_sobelx = np.sqrt(sobelx**2)\n",
    "    sqrt_sobely = np.sqrt(sobely**2)\n",
    "    abs_sobelx = np.absolute(sqrt_sobelx)\n",
    "    abs_sobely = np.absolute(sqrt_sobely)\n",
    "    \n",
    "    # 4) Scale to 8-bit (0 - 255) and convert to type = np.uint8\n",
    "    scaled_sobel = np.uint8(255*abs_sobelx/(2*np.max(abs_sobelx)))+ np.uint8(255*abs_sobely/(2*np.max(abs_sobely)))\n",
    "    scaled_s_channel = np.uint8(255*s_channel/np.max(s_channel))\n",
    "        \n",
    "    # 5) Create a binary mask where mag thresholds are met\n",
    "\n",
    "    # Threshold x gradient\n",
    "    #binary_output -->sxbinary\n",
    "    sxbinary = np.zeros_like(scaled_sobel)\n",
    "    sxbinary[(scaled_sobel >= sx_thresh[0]) & (scaled_sobel <= sx_thresh[1])] = 1\n",
    "\n",
    "    # Threshold color channel\n",
    "    s_binary = np.zeros_like(s_channel)\n",
    "    s_binary[(s_channel >= s_thresh[0]) & (s_channel <= s_thresh[1])] = 1\n",
    "\n",
    "    # Stack each channel\n",
    "    # Note color_binary[:, :, 0] is all 0s, effectively an all black image. It might\n",
    "    # be beneficial to replace this channel with something else.\n",
    "    color_binary = np.dstack(( np.zeros_like(sxbinary), sxbinary, s_binary))\n",
    "    color_binary[(color_binary >=1)] = 255\n",
    "\n",
    "    # 6) Return this mask as your binary_output image\n",
    "\n",
    "    return color_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bird_view_transform(undist_road_view, corners):         \n",
    "    #undist is an image that has been previously undistorted using the computed coefs of the camera lense\n",
    "    src = corners\n",
    "    offset = 100\n",
    "    #define 4 destination points dst = np.float32([[,],[,],[,],[,]])\n",
    "    dst = np.float32([[offset, offset],[img_size[0]-offset,offset],[img_size[0]-offset,img_size[1]-offset],[offset, img_size[1] - offset]])\n",
    "    # use cv2.getPerspectiveTransform() to get M, the transform matrix and Minv, the inverse transform matrix.\n",
    "    M = cv2.getPerspectiveTransform(src, dst)\n",
    "    Minv = cv2.getPerspectiveTransform(dst, src)\n",
    "    # use cv2.warpPerspective() to warp your image to a top-down view\n",
    "    warped_to_birdview = cv2.warpPerspective(undist_road_view, M, img_size, flags=cv2.INTER_LINEAR)\n",
    "    \n",
    "    return warped_to_birdview, M, Minv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def road_view_transform(warped_birdview, Minv_road, img_size):\n",
    "    #basic reverse function, may need tuning\n",
    "    \n",
    "    warped_to_roadview = cv2.warpPerspective(warped_birdview, Minv_road, img_size, flags=cv2.INTER_LINEAR)\n",
    "    \n",
    "    return warped_to_roadview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions copied from Udacity class exercise material#\n",
    "\n",
    "def window_mask(width, height, birdview_img_ref, center,level):\n",
    "    output_masked = np.zeros_like(birdview_img_ref)\n",
    "    output_masked[int(birdview_img_ref.shape[0]-(level+1)*height):int(birdview_img_ref.shape[0]-level*height),max(0,int(center-width/2)):min(int(center+width/2),birdview_img_ref.shape[1])] = 1\n",
    "    return output_masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_window_centroids(image_to_search, window_width, window_height, margin,prev_lx,prev_rx):\n",
    "    \n",
    "    window_centroids_found = [] # Store the (left,right) window centroid positions per level\n",
    "    window = np.ones(window_width) # Create our window template that we will use for convolutions\n",
    "    \n",
    "    # First find the two starting positions for the left and right lane by using np.sum to get the vertical image slice\n",
    "    # and then np.convolve the vertical image slice with the window template \n",
    "    search_window_width = window_width\n",
    "    #check if lines have been previously found, and define x range to search for left line and right line\n",
    "    if (prev_lx < search_window_width):# to consider that no previous left lane location is known\n",
    "        # Sum quarter bottom of image to get slice, could use a different ratio\n",
    "        l_sum = np.sum(image_to_search[int(3*image_to_search.shape[0]/4):,:int(image_to_search.shape[1]/2)], axis=0)\n",
    "        #l_center = np.argmax(np.convolve(window,l_sum))-window_width/2\n",
    "    else:\n",
    "        l_sum_xmin = int(prev_lx - search_window_width)\n",
    "        l_sum_xmax = int(prev_lx + search_window_width)\n",
    "        l_sum = np.sum(image_to_search[l_sum_xmin:l_sum_xmax,:int(image_to_search.shape[1]-window_height)], axis=0)\n",
    "    \n",
    "    l_center = np.argmax(np.convolve(window,l_sum))-window_width/2\n",
    "        \n",
    "    if ((prev_rx < int(image_to_search.shape[1]/2))or(prev_rx > int(image_to_search.shape[1]-search_window_width-1))):\n",
    "        #the minus 1 at the end is because the indexes of the image go from 0 to image.size[x]-1, and here \n",
    "        #the risk is to try performing operations using pixels that are out of range of the image to search\n",
    "        #Sum quarter bottom right hand side of the image, considering no right lane line was detected previously\n",
    "        r_sum = np.sum(image_to_search[int(3*image_to_search.shape[0]/4):,int(image_to_search.shape[1]/2):], axis=0)\n",
    "        \n",
    "    else:\n",
    "        r_sum_xmin = int(prev_rx - search_window_width)\n",
    "        r_sum_xmax = int(prev_rx + search_window_width)\n",
    "        r_sum = np.sum(image_to_search[r_sum_xmin:r_sum_xmax,:int(image_to_search.shape[1]-window_height)], axis=0)\n",
    "        \n",
    "    r_center = np.argmax(np.convolve(window,r_sum))-window_width/2+int(image_to_search.shape[1]/2)\n",
    "                                         \n",
    "    # Sum quarter bottom of image to get slice, could use a different ratio //this is the original code snippet, without any additional tracking of previous lane positions\n",
    "    #l_sum = np.sum(image[int(3*image.shape[0]/4):,:int(image.shape[1]/2)], axis=0)\n",
    "    #l_center = np.argmax(np.convolve(window,l_sum))-window_width/2\n",
    "    #r_sum = np.sum(image_to_search[int(3*image_to_search.shape[0]/4):,int(image_to_search.shape[1]/2):], axis=0)\n",
    "    #r_center = np.argmax(np.convolve(window,r_sum))-window_width/2+int(image_to_search.shape[1]/2)\n",
    "    \n",
    "    # Add what we found for the first layer\n",
    "    window_centroids_found.append((l_center,r_center))\n",
    "    \n",
    "    # Go through each layer looking for max pixel locations\n",
    "    for level in range(1,(int)(image_to_search.shape[0]/window_height)):\n",
    "        # convolve the window into the vertical slice of the image\n",
    "        image_layer = np.sum(image_to_search[int(image_to_search.shape[0]-(level+1)*window_height):int(image_to_search.shape[0]-level*window_height),:], axis=0)\n",
    "        conv_signal = np.convolve(window, image_layer)\n",
    "        # Find the best left centroid by using past left center as a reference\n",
    "        # Use window_width/2 as offset because convolution signal reference is at right side of window, not center of window\n",
    "        offset = window_width/2\n",
    "        l_min_index = int(max(l_center+offset-margin,0))\n",
    "        l_max_index = int(min(l_center+offset+margin,image_to_search.shape[1]))\n",
    "        l_center = np.argmax(conv_signal[l_min_index:l_max_index])+l_min_index-offset\n",
    "        # Find the best right centroid by using past right center as a reference\n",
    "        r_min_index = int(max(r_center+offset-margin,0))\n",
    "        r_max_index = int(min(r_center+offset+margin,image_to_search.shape[1]))\n",
    "        r_center = np.argmax(conv_signal[r_min_index:r_max_index])+r_min_index-offset\n",
    "        # Add what we found for that layer\n",
    "        window_centroids_found.append((l_center,r_center))\n",
    "\n",
    "    return window_centroids_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly_image_extractor(window_centroids,image, level):\n",
    "    \n",
    "    img_height = image.shape[0]\n",
    "    img_width = image.shape[1]\n",
    "    # Create empty lists to receive left and right lane pixel indices for x and y axes\n",
    "    left_lane_inds = []\n",
    "    right_lane_inds = []\n",
    "    leftx=[]\n",
    "    lefty=[]\n",
    "    rightx=[]\n",
    "    righty=[]\n",
    "    leftyx=[]\n",
    "    rightyx=[]\n",
    "\n",
    "    for level in range((int)(len(window_centroids))):\n",
    "        \n",
    "        if (window_centroids[level][0]!=0):\n",
    "            lx=int(window_centroids[level][0])\n",
    "            ly=int((img_height-level*80-40))\n",
    "            if ((lx <=img_width)&(ly<=img_height)):\n",
    "                left_lane_inds.append(lx)\n",
    "                leftx.append(lx) #appending here the pixel in image width\n",
    "                lefty.append(ly) #appending here the pixel in image height, computed from the image height minus the window level and taken at the center of the window (50pix wide, 80 high)\n",
    "                leftyx.append((ly,lx))\n",
    "                \n",
    "        if (window_centroids[level][1]!=0):\n",
    "            rx = int(window_centroids[level][1])\n",
    "            ry=int(img_height-level*80-40)\n",
    "            if ((rx <=img_width)&(ry<=img_height)):\n",
    "                right_lane_inds.append(rx)\n",
    "                rightx.append(rx) #appending here the pixel in image width\n",
    "                righty.append(ry) #appending here the pixel in image height, computed from the image height minus the window level and taken at the center of the window (50pix wide, 80 high)\n",
    "                rightyx.append((ry, rx))\n",
    "\n",
    "    # Fit a second order polynomial to each\n",
    "    left_fit = np.polyfit(lefty, leftx, 2)\n",
    "    right_fit = np.polyfit(righty, rightx, 2)\n",
    "\n",
    "    # Generate x and y values for plotting\n",
    "    ploty = np.linspace(0, image.shape[0]-1, image.shape[0] )\n",
    "    left_fitx = left_fit[0]*ploty**2 + left_fit[1]*ploty + left_fit[2]\n",
    "    right_fitx = right_fit[0]*ploty**2 + right_fit[1]*ploty + right_fit[2]\n",
    "    \n",
    "    \n",
    "    #this is a temporary fix to getting array indices out of range (and non-plottable to the image, without error)\n",
    "    #I would like to simply \"erase\" the index to this section of the fitted polynom\n",
    "    for ix in range((int)(len(left_fitx))):\n",
    "        if (left_fitx[ix] < 0 ):\n",
    "            left_fitx[ix]= left_fit[0]*(0.5*img_width)**2+left_fit[1]*(0.5*img_width)+left_fit[1]\n",
    "            \n",
    "        \n",
    "    for ix in range((int)(len(right_fitx))):\n",
    "        if(right_fitx[ix] > img_width):\n",
    "            right_fitx[ix] = right_fit[0]*(0.5*img_width)**2+right_fit[1]*(0.5*img_width)+right_fit[1]\n",
    "\n",
    "    ploty.astype(int)\n",
    "    ploty_int = ploty.astype(int)\n",
    "    left_fitx_int = left_fitx.astype(int)\n",
    "    right_fitx_int = right_fitx.astype(int)\n",
    "\n",
    "    # Create an image to draw on and an image to show the selection window\n",
    "    polyfit_img = np.zeros_like(image)\n",
    "    color_polyfit_img = np.dstack((polyfit_img, polyfit_img, polyfit_img))\n",
    "    \n",
    "    # Recast the x and y points into usable format for cv2.fillPoly()\n",
    "    pts_left = np.array([np.transpose(np.vstack([left_fitx_int, ploty_int]))])\n",
    "    pts_right = np.array([np.flipud(np.transpose(np.vstack([right_fitx_int, ploty_int])))])\n",
    "    pts = np.hstack((pts_left, pts_right))\n",
    "    pts_int =pts.astype(int)\n",
    "\n",
    "    # Draw the lane onto the warped blank image\n",
    "    #cv2.fillPoly(color_polyfit_img, pts_int, (0,255, 0))\n",
    "    \n",
    "    # Color in left and right line pixels\n",
    "    #polyfit_img[(lefty,leftx)] = [255, 255, 255]\n",
    "    #polyfit_img[(righty,rightx)] = [255, 255, 255]\n",
    "    polyfit_img[(ploty_int,left_fitx_int)] = [255,0,255]\n",
    "    polyfit_img[(ploty_int,right_fitx_int)] = [255,0,0]\n",
    "    #polyfit_img = cv2.addWeighted(color_polyfit_img, 1, image, 0.3, 0)\n",
    "    #print ('poly fit end')\n",
    "    \n",
    "    #image_to_search[int(3*image_to_search.shape[0]/4):,:int(image_to_search.shape[1]/2)]\n",
    "    \n",
    "    #polyfit_img2= window_mask(width=10, height=10, polyfit_img, center=left_fitx_int,level=nbstrips)\n",
    "    #polyfit_img2= window_mask(width=10, height=10, polyfit_img, center=right_fitx_int,level=nbstrips)\n",
    "       \n",
    "    #polyfit_img2= window_mask(10, 10, polyfit_img, left_fitx_int,nb_strips)\n",
    "    #polyfit_img2= window_mask(10, 10, polyfit_img, right_fitx_int,nb_strips)\n",
    " \n",
    "    #in this section of the code, we will compute the radius of the curvature, answer given in meters.\n",
    "    #code taken from course material\n",
    "    # Define conversions in x and y from pixels space to meters\n",
    "    ym_per_pix = (float)(3/56) # (30/720) meters per pixel in y dimension\n",
    "    xm_per_pix = (float)(3.7/676) # (3.7/700) meters per pixel in x dimension\n",
    "    lefty_ws = []\n",
    "    leftx_ws = []\n",
    "    righty_ws = []\n",
    "    rightx_ws = []\n",
    "    \n",
    "    # Fit new polynomials to x,y in world space\n",
    "    # Fit a second order polynomial to each, in world space\n",
    "    for inx in range(0,len(lefty)):\n",
    "        lefty_ws.append(lefty[inx]*ym_per_pix)\n",
    "    \n",
    "    for inx in range(0,len(leftx)):\n",
    "        leftx_ws.append(leftx[inx]*xm_per_pix)\n",
    "    \n",
    "    for inx in range(0,len(righty)):\n",
    "        righty_ws.append(righty[inx]*ym_per_pix)\n",
    "    \n",
    "    for inx in range(0,len(rightx)):\n",
    "        rightx_ws.append(rightx[inx]*xm_per_pix)\n",
    "    \n",
    "    left_fit_cr = np.polyfit(lefty_ws, leftx_ws, 2)\n",
    "    right_fit_cr = np.polyfit(righty_ws, rightx_ws, 2)\n",
    "\n",
    "    # Calculate the new radii of curvature\n",
    "    y_eval = np.max(ploty_int)\n",
    "    y_eval_m =y_eval*ym_per_pix # evaluation of y near the car (bottom of the picture)\n",
    "\n",
    "    left_curverad = ((1 + (2*left_fit[0]*y_eval + left_fit[1])**2)**1.5) / np.absolute(2*left_fit[0])\n",
    "    right_curverad = ((1 + (2*right_fit[0]*y_eval + right_fit[1])**2)**1.5) / np.absolute(2*right_fit[0])\n",
    "    \n",
    "    left_curverad_m = ((1 + (2*left_fit_cr[0]*y_eval_m + left_fit_cr[1])**2)**1.5) / np.absolute(2*left_fit_cr[0])\n",
    "    right_curverad_m = ((1 + (2*right_fit_cr[0]*y_eval_m + right_fit_cr[1])**2)**1.5) / np.absolute(2*right_fit_cr[0])\n",
    "    # Now our radius of curvature is in meters\n",
    "    \n",
    "    #print('left lane estimate poly in pixels:' + str(round(left_fit[0],6)) + '*x^2 +' + str(round(left_fit[1],6)) + '*x +'+ str(round(left_fit[2],6)))\n",
    "    #print('right lane estimate poly in pixels:' + str(round(right_fit[0],6)) + '*x^2 +' + str(round(right_fit[1],6)) + '*x +'+ str(round(right_fit[2],6)))\n",
    "    \n",
    "    #print('left lane estimate poly in meters:' + str(round(left_fit[0]*ym_per_pix,6)) + '*x^2 +' + str(round(left_fit[1],6)) + '*x +'+ str(round(left_fit[2],2)))\n",
    "    #print('right lane estimate poly in meters:' + str(round(right_fit[0]*ym_per_pix,6)) + '*x^2 +' + str(round(right_fit[1],6)) + '*x +'+ str(round(right_fit[2],2)))\n",
    "\n",
    "    \n",
    "    #print('evaluated curvature : \\n',left_curverad, 'm', right_curverad, 'm')\n",
    "    # Example values: 632.1 m    626.2 m\n",
    "    \n",
    "    # Calculate the position of the car with respect to the center of the lane:\n",
    "    # max index\n",
    "    left_lane_pos = left_fitx_int[(len(left_fitx_int) - 1)]\n",
    "    right_lane_pos = right_fitx_int[(len(right_fitx_int) - 1)]\n",
    "    \n",
    "    #not robust here, since it can fail if either right lane or left lane is not found in the current image\n",
    "    lane_width_pix = right_lane_pos - left_lane_pos\n",
    "    lane_width_m = lane_width_pix * xm_per_pix\n",
    "    mid_lane_pos = int(lane_width_pix/2)\n",
    "    car_pos = int(img_width/2)-mid_lane_pos\n",
    "    car_pos_m = car_pos*xm_per_pix\n",
    "    \n",
    "    #print('lane width in pixels : ', round(lane_width_pix,2))\n",
    "    #print('position of the middle of the lane in pixels : ', round(mid_lane_pos,2))\n",
    "    #print('position of the car with respect to the lane in pixels : ', round(car_pos,2))\n",
    "    #print('lane width in meters : ', round(lane_width_m,2))\n",
    "    #print('position of the car with respect to the lane center in meters : ', round(car_pos_m,2))\n",
    "    \n",
    "    text_print = '\\n'\n",
    "    #text_print += 'left lane estimate poly in meters:' + str(round(left_fit[0]*ym_per_pix,6)) + '*x^2 +' + str(round(left_fit[1],6)) + '*x +'+ str(round(left_fit[2],2))+'\\n'\n",
    "    #text_print +='right lane estimate poly in meters:' + str(round(right_fit[0]*ym_per_pix,6)) + '*x^2 +' + str(round(right_fit[1],6)) + '*x +'+ str(round(right_fit[2],2))+'\\n'\n",
    "    text_print += 'evaluated curvature left: '+str(left_curverad)+ 'm \\nevaluated curvature right: '+ str(right_curverad)+ 'm'\n",
    "    text_print += 'lane width in meters : '+ str(round(lane_width_m,2)) +'\\n'\n",
    "    text_print += 'position of the car with respect to the lane center in meters : '+ str(round(car_pos_m,2))+ '\\n'\n",
    "    \n",
    "    text_print=(''.join(['Curvature Radius left = ', str(left_curverad), ' meters']))\n",
    "    \n",
    "    return ploty_int,left_fit, right_fit, polyfit_img,text_print #result (replaced by out_img)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions copied from Udacity class exercise material#\n",
    "\n",
    "def template_creator(window_centroids, warped_image, window_width, window_height, margin):\n",
    "    # If we found any window centers\n",
    "    if len(window_centroids) > 0:\n",
    "    \n",
    "        # Points used to draw all the left and right windows\n",
    "        l_points = np.zeros_like(warped_image)\n",
    "        r_points = np.zeros_like(warped_image)\n",
    "        \n",
    "        #print('window_centroids[0][0]: ',window_centroids[0][0])\n",
    "        #print('window_centroids[0][1]: ',window_centroids[0][1])\n",
    "        \n",
    "        # Go through each level and draw the windows \t\n",
    "        for level in range(0,len(window_centroids)):\n",
    "            # Window_mask is a function to draw window areas\n",
    "            l_mask = window_mask(window_width,window_height,warped_image,window_centroids[level][0],level)\n",
    "            r_mask = window_mask(window_width,window_height,warped_image,window_centroids[level][1],level)\n",
    "            # Add graphic points from window mask here to total pixels found \n",
    "            l_points[(l_points == 255) | ((l_mask == 1) ) ] = 255\n",
    "            r_points[(r_points == 255) | ((r_mask == 1) ) ] = 255\n",
    "\n",
    "        # Draw the results\n",
    "        template_r = np.array(r_points,np.uint8) # np.array(r_points+l_points,np.uint8) # add both left and right window pixels together\n",
    "        template_l = np.array(l_points,np.uint8)\n",
    "        zero_channel = np.zeros_like(template_r) # create a zero color channel\n",
    "        template = np.array(cv2.merge((zero_channel,template_r,template_l)),np.uint8) # make window pixels green for right pixels\n",
    "        warpage = np.array(cv2.merge((warped_image,warped_image,warped_image)),np.uint8) # making the original road pixels 3 color channels\n",
    "        warped_template = cv2.addWeighted(warpage, 1.5, template, 0.5, 0.0) # overlay the orignal road image with window results\n",
    "        \n",
    "        level=len(window_centroids)\n",
    "        ploty_fit, left_fit, right_fit, resulting_polyfit_template, text_to_print = poly_image_extractor(window_centroids, warped_template, level) \n",
    "        #resulting_template[lefty,leftx,:] = [255, 0, 0]\n",
    "        #resulting_template[righty,rightx,:] = [0, 0, 255]\n",
    "        #plt.imshow(resulting_template)\n",
    "        #plt.plot(left_fit, ploty, color='yellow')\n",
    "      \n",
    "        \n",
    "    # If no window centers found, just display orginal road image\n",
    "    else:\n",
    "        resulting_polyfit_template = np.array(cv2.merge((warped_image,warped_image,warped_image)),np.uint8)\n",
    "    \n",
    "    return template, warpage, resulting_polyfit_template, text_to_print\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### C:\\Users\\meile\\Projects\\CarND-Project4\\CarND-Advanced-Lane-Lines-master\\test_images\\test3.jpg ####\n",
      "Curvature Radius left = 685.576344119 meters\n",
      "################\n"
     ]
    }
   ],
   "source": [
    "#test the functions proivded above\n",
    "\n",
    "#path name for test images\n",
    "pathname2 = 'C:\\\\Users\\\\meile\\\\Projects\\\\CarND-Project4\\\\CarND-Advanced-Lane-Lines-master\\\\test_images\\\\'\n",
    "\n",
    "test_images = glob.glob(pathname2 + '*.jpg')\n",
    "\n",
    "\n",
    "#these are the corners identified in the picture to perform the bird view transform (width, height), \n",
    "#starting with the top right corner, top left corner, bottom left corner, bottom right corner\n",
    "\n",
    "corners = np.float32(\n",
    "    [[585,450],\n",
    "     [715,450],\n",
    "     [1210,600],\n",
    "     [360,600]])\n",
    "\n",
    "#parameters for defining the window search are set here\n",
    "window_width = 50 \n",
    "window_height = 80 # Break image into 9 vertical layers since image height is 720\n",
    "margin = 100 # How much to slide left and right for searching\n",
    "\n",
    "\n",
    "for fname in test_images:\n",
    "    # using the camera matrix corrections saved in the object newcameramtx_cam to undistort the image fname.\n",
    "    # original_road_image : original image before correcting for the camera lense distortion\n",
    "    # undistorted_road_image : image after correcting for distortion\n",
    "    # pipelined_road_binary : the image result from applying soebel functions to search for gradients in the undistorted_road_image\n",
    "    # warped_road_image : sets the pipelined_road_image to a binary image (0 or 255) by adding the pixels from each color channel of the pipelined_road_image together\n",
    "    # warped_road_image_birdview : this is the resulting image of perspective transform of the warped_road_image. It is the image used to search for window centroids\n",
    "    # template_created : this is a mask that plots where window centroids were found in the warped_road_image_birdview\n",
    "    \n",
    "    original_road_image, undistorted_road_image = correct_dist(fname, newcameramtx_cam, dist_cam, mtx_cam) \n",
    "    gray = cv2.cvtColor(undistorted_road_image,cv2.COLOR_BGR2GRAY)\n",
    "    img_size = gray.shape[::-1]\n",
    "    \n",
    "    pipelined_road_binary = pipeline(undistorted_road_image, s_thresh, sx_thresh)\n",
    "    \n",
    "    ###this hereafter needs to be changed to improve my algorithme:####\n",
    "    warped_road_image=pipelined_road_binary[:,:,0]+pipelined_road_binary[:,:,1]+pipelined_road_binary[:,:,2]\n",
    "    warped_road_image_birdview, M_road, Minv_road = bird_view_transform(warped_road_image, corners)\n",
    "    #print(warped2.shape)\n",
    "    previous_lx = 0#820#162\n",
    "    previous_rx = 0#160#824\n",
    "    window_centroids = find_window_centroids(warped_road_image_birdview, window_width, window_height, margin, previous_lx, previous_rx)\n",
    "    template_created, warpage_created, resulting_polyfit_template_created, resulting_text_to_print = template_creator(window_centroids, warped_road_image_birdview, window_width, window_height, margin)\n",
    "    warped_to_roadview_template = road_view_transform(resulting_polyfit_template_created, Minv_road, img_size)\n",
    "    \n",
    "    undistorted_road_image_lined = cv2.addWeighted(undistorted_road_image, 1.0, warped_to_roadview_template, 10.5, 0.0)\n",
    "    print('####',fname,'####')\n",
    "    print(resulting_text_to_print)\n",
    "    print('################')\n",
    "    \n",
    "    \n",
    "    # Display the final results\n",
    "    \n",
    "\n",
    "    #cv2.imshow('original_road_image',original_road_image)\n",
    "    #cv2.imshow('undistorted_road_image',undistorted_road_image)\n",
    "    #cv2.imshow('pipelined_road_binary',pipelined_road_binary)\n",
    "    #cv2.imshow('warped_road_image',warped_road_image)\n",
    "    #cv2.imshow('warped_road_image_birdview',warped_road_image_birdview)\n",
    "    #cv2.imshow('template_created',template_created)\n",
    "    #cv2.imshow('warpage_created',warpage_created)\n",
    "    #cv2.imshow('resulting_polyfit_template_created',resulting_polyfit_template_created)\n",
    "    #cv2.imshow('warped_to_roadview_template',warped_to_roadview_template)\n",
    "    #cv2.imshow('undistorted_road_image_lined',undistorted_road_image_lined)\n",
    "    #cv2.waitKey(500)\n",
    "    \n",
    "    #cv2.waitKey(0)\n",
    "    #cv2.destroyAllWindows()\n",
    "    \n",
    "    save_name = fname + 'original_road_image.jpg'\n",
    "    cv2.imwrite(save_name,original_road_image)\n",
    "    save_name = fname + 'undistorted_road_image.jpg'\n",
    "    cv2.imwrite(save_name,undistorted_road_image)\n",
    "    save_name = fname + 'pipelined_road_binary.jpg'\n",
    "    cv2.imwrite(save_name,pipelined_road_binary)\n",
    "    save_name = fname + 'warped_road_imagepipelined_road_binary.jpg'\n",
    "    cv2.imwrite(save_name,warped_road_image)\n",
    "    save_name = fname + 'warped_road_image_birdview.jpg'\n",
    "    cv2.imwrite(save_name,warped_road_image_birdview)\n",
    "    save_name = fname + 'template_created.jpg'\n",
    "    cv2.imwrite(save_name,template_created)\n",
    "    save_name = fname + 'warpage_created.jpg'\n",
    "    cv2.imwrite(save_name,warpage_created)\n",
    "    save_name = fname + 'resulting_polyfit_template_created.jpg'\n",
    "    cv2.imwrite(save_name,resulting_polyfit_template_created)\n",
    "    save_name = fname + 'warped_to_roadview_template.jpg'\n",
    "    cv2.imwrite(save_name,warped_to_roadview_template)\n",
    "    save_name = fname + 'undistorted_road_image_lined.jpg'\n",
    "    cv2.imwrite(save_name,undistorted_road_image_lined)\n",
    "    \n",
    "    \n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    cv2.putText(undistorted_road_image_lined,resulting_text_to_print,(10,500), font, 0.6,(255,255,255),2,cv2.LINE_AA)\n",
    "    #cvInitFont(CvFont* font, int font_face, double hscale, double vscale, double shear=0, int thickness=1, int line_type=8 )\n",
    "    #cv2.putText(undistorted_road_image_lined,resulting_text_to_print,hscale=0.5)\n",
    "    \n",
    "    save_name = fname + \"undistorted_road_image_lined_commented.jpg\"  #<-- this is a dirty workaround for image renaming.            \n",
    "    cv2.imwrite(save_name,undistorted_road_image_lined)\n",
    "\n",
    "\n",
    "    #print('-----------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section provides the code for video editing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import everything needed to edit/save/watch video clips\n",
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML\n",
    "\n",
    "video_path='C:\\\\Users\\\\meile\\\\Projects\\\\CarND-Project4\\\\CarND-Advanced-Lane-Lines-master\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(video_image):\n",
    "    corners = np.float32(\n",
    "    [[585,450],\n",
    "     [715,450],\n",
    "     [1210,600],\n",
    "     [360,600]])\n",
    "\n",
    "    #parameters for defining the window search are set here\n",
    "    window_width = 50 \n",
    "    window_height = 80 # Break image into 9 vertical layers since image height is 720\n",
    "    margin = 100 # How much to slide left and right for searching\n",
    "\n",
    "    # using the camera matrix corrections saved in the object newcameramtx_cam to undistort the image fname.\n",
    "    # original_road_image : original image before correcting for the camera lense distortion\n",
    "    # undistorted_road_image : image after correcting for distortion\n",
    "    # pipelined_road_binary : the image result from applying soebel functions to search for gradients in the undistorted_road_image\n",
    "    # warped_road_image : sets the pipelined_road_image to a binary image (0 or 255) by adding the pixels from each color channel of the pipelined_road_image together\n",
    "    # warped_road_image_birdview : this is the resulting image of perspective transform of the warped_road_image. It is the image used to search for window centroids\n",
    "    # template_created : this is a mask that plots where window centroids were found in the warped_road_image_birdview\n",
    "    \n",
    "    original_road_image, undistorted_road_image = correct_dist_video(video_image, newcameramtx_cam, dist_cam, mtx_cam) \n",
    "    gray = cv2.cvtColor(undistorted_road_image,cv2.COLOR_BGR2GRAY)\n",
    "    img_size = gray.shape[::-1]\n",
    "    \n",
    "    pipelined_road_binary = pipeline(undistorted_road_image, s_thresh, sx_thresh)\n",
    "    \n",
    "    ###this hereafter needs to be changed to improve my algorithme:####\n",
    "    warped_road_image=pipelined_road_binary[:,:,0]+pipelined_road_binary[:,:,1]+pipelined_road_binary[:,:,2]\n",
    "    warped_road_image_birdview, M_road, Minv_road = bird_view_transform(warped_road_image, corners)\n",
    "    #no automatic search function is implemented here, because the search filter did not provide satisfactory results\n",
    "    previous_lx = 0\n",
    "    previous_rx = 0\n",
    "    window_centroids = find_window_centroids(warped_road_image_birdview, window_width, window_height, margin, previous_lx, previous_rx)\n",
    "    template_created, warpage_created, resulting_polyfit_template_created, resulting_text_to_print = template_creator(window_centroids, warped_road_image_birdview, window_width, window_height, margin)\n",
    "    warped_to_roadview_template = road_view_transform(resulting_polyfit_template_created, Minv_road, img_size)\n",
    "    \n",
    "    undistorted_road_image_lined = cv2.addWeighted(undistorted_road_image, 1.0, warped_to_roadview_template, 10.5, 0.0)\n",
    "    cv2.putText(img=np.copy(undistorted_road_image_lined), text=(''.join([resulting_text_to_print])), org=(100,100),fontFace=5, fontScale=1, color=(255,255,255), thickness=1)\n",
    "    \n",
    "    return undistorted_road_image_lined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\meile\\Projects\\CarND-Project4\\CarND-Advanced-Lane-Lines-master\\project_video_output.mp4\n",
      "C:\\Users\\meile\\Projects\\CarND-Project4\\CarND-Advanced-Lane-Lines-master\\project_video.mp4\n",
      "[MoviePy] >>>> Building video C:\\Users\\meile\\Projects\\CarND-Project4\\CarND-Advanced-Lane-Lines-master\\project_video_output.mp4\n",
      "[MoviePy] Writing video C:\\Users\\meile\\Projects\\CarND-Project4\\CarND-Advanced-Lane-Lines-master\\project_video_output.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████▉| 1260/1261 [06:02<00:00,  3.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: C:\\Users\\meile\\Projects\\CarND-Project4\\CarND-Advanced-Lane-Lines-master\\project_video_output.mp4 \n",
      "\n",
      "Wall time: 6min 3s\n"
     ]
    }
   ],
   "source": [
    "white_output = video_path+'project_video_output.mp4'\n",
    "white_input = video_path+'project_video.mp4'\n",
    "print(white_output)\n",
    "print(white_input)\n",
    "clip1 = VideoFileClip(white_input)\n",
    "white_clip = clip1.fl_image(process_image) #NOTE: this function expects color images!!\n",
    "get_ipython().magic(u'time white_clip.write_videofile(white_output, audio=False)')\n",
    "\n",
    "#video_clip.reader.close()\n",
    "#video_clip.audio.reader.close_proc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"960\" height=\"540\" controls>\n",
       "  <source src=\"C:\\Users\\meile\\Projects\\CarND-Project4\\CarND-Advanced-Lane-Lines-master\\project_video_output.mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(white_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
